Разработка

 Обоснование средств разработки

Для того чтобы успешно выполнить поставленную задачу в соответствии с техническим заданием, мы должны определить основные программные средства разработки и алгоритмы функционирования приложения.
Выбор предметной области накладывает некоторые, хотя и не слишком критические, ограничения на выбор таких технических средств как среда разработки, язык программирования, СУБД, web-фреймворки и т.п. В нашем случае, можно сформулировать два основных требования к языку разработки.
Во-первых, характер поставленной задачи предполагает, что для её достижения необходимо реализовать определённый алгоритм обработки текста. Это в свою очередь означает, что необходимо реализовать математические алгоритмы, составляющие основу выбранного метода текстового анализа, что в свою очередь представляет собой достаточно сложную вычислительную задачу.
Во-вторых, выбранный язык программирования должен не просто позволять решать поставленную задачу, но делать её решение наиболее простым и понятным как для программиста, так и для конечного пользователя итогового приложения.
Именно поэтому, в качестве языка разработки был избран язык Python. Несмотря на то, что python является языком программирования общего назначения, он полностью удовлетворяет условиям, выдвинутым к средствам разработки в данной курсовой работе. Конечно можно заметить, что для реализации специфических математических алгоритмов больше подошли бы специализированные языки, например, язык R, или даже целые программные пакеты вроде Matlab или Maple. Но, поскольку целью работы является не только исследование алгоритма анализа текстов, но и написание программного продукта, то от этого варианта была решено отказаться.
Кроме того, python изначально был ориентирован на повышение производительности разработчика и читаемости кода. Это отвечает нашему второму требованию к языку разработки. Для python существует огромное количество разнообразных пакетов и библиотек, реализующих математические методы, являющиеся частью алгоритма текстового анализа. Подробнее об этих методах и их применении написано в следующем пункте данного раздела.
Также важным является выбор среды разработки. Для Python существует целый набор мощных IDE, некоторые из которых позиционируют себя как специализированные среды для разработки научных и исследовательских приложений. Таковы, например, Python (x, y), Spyder (IDE) и другие. Однако, в качестве среды разработки нами была выбрана IDE PyCharm и это вполне объяснимо.
Как и при выборе языка, здесь большую роль сыграло удобство разработки. Ведь по большому счёту программы на Python можно писать и в блокноте, пользуясь консолью для запуска интерпретатора. PyCharm имеет удобный, настраиваемый интерфейс, инструменты для рефакторинга и анализа кода, что уже само по себе повышает эффективность разработки. Установка дополнительных пакетов от IDE, в принципе, не зависит и одинаково проста для всех сред.
О библиотеках и пакетах, использовавшихся при создании проекта, будет подробно рассказано в дальнейшем. Однако следует заметить, что почти все они входят в состав платформы Anaconda, включающей в себя набор библиотек для научных и инженерных расчетов, которая и была использована для создания приложения.
На этом обоснование средств разработки можно считать законченным, и переходить к описанию самого процесса разработки.
 
 Алгоритм латентно-семантического анализа

В качестве основного инструмента данной курсовой работы выступает латентно-семантический анализ. И прежде чем переходить к описанию его работы следует обосновать такой выбор.
ЛСА позволяет находить взаимосвязь между документами и терминами, встречающимися в них, ставить некоторую тематику в соответствие документу. Причём документом может выступать как самостоятельный текст, так и часть текста, например, абзац. Именно эти свойства и послужили ключевыми факторами при выборе. Вообще, работу алгоритма можно сравнить с простейшей нейросетью. В ней три слоя: первый содержит множество термов, второй – набор документов, а третий – скрытый промежуточный слой с весовыми коэффициентами, связывает первые два. Такая организация работы позволяет говорить не просто о реализации алгоритма, но и о построении простейшей системы искусственного интеллекта.
Исходной информацией для ЛСА служит матрица термы-на-документы, содержащая частоты встречаемости тех или иных слов в документах. Наиболее распространённый вариант анализа заключается в том, что эта матрица подвергается сингулярному разложению. Полученная на выходе матрица будет отражать основную структуру различных зависимостей, присутствующих в исходной матрице и, следовательно, структуру зависимостей между документами и термами.
Выбранный алгоритм обладает рядом достоинств. Он признан одним из лучших методов для выявления скрытых связей в наборе документов, даже если статистические методы с этим не справляются, что полностью соответствует поставленной задаче. Также ЛСА может применяться как с использованием обучения, так и без него, что как упрощает данную работу, так и позволяет наметить пути совершенствования разрабатываемого приложения. Наконец, метод позволяет игнорировать такие лексические особенности как многозначность слов, что идёт на пользу итоговым результатам.
Конечно ЛСА не лишён и недостатков, самым существенным из которых является небольшая скорость работы алгоритма и значительное её снижение при увеличении объёма входных данных. Скорость вычисления находится по следующей формуле (1), где V – скорость, k – размерность матрицы термы-на-документы, а Ndoc и Nterm – количество документов и термов соответственно.

〖V=N〗^(2*k),N=N_doc+N_term;	(1)

Кроме того, в процессе вычислений предполагается, что термы и документы имеют нормальное распределение, однако это не соответствует действительности. Ближе всего к реальному – распределение Пуассона, в связи с чем для практического применения лучше подходит вероятностный латентно-семантический анализ (ВЛСА). Этот подход считается более обоснованным, но не даёт возможности получить детерминированные результаты – они каждый раз будут отличаться, хотя и незначительно.
Однако недостатки простого ЛСА можно сгладить, если использовать дополнительные алгоритмы. Например, метод главных компонент, который позволит отобрать только самые важные признаки из матриц, полученных в ходе разложения. Это позволит повысить эффективность итоговой классификации.
В силу этого в качестве основного алгоритма в данном исследовании был избран ЛСА с некоторыми усовершенствованиями. Используя этот метод можно добиться наиболее понятных и чётких результатов по сравнению с аналогами.
 
 Первичная обработка текста

Прежде чем переходить непосредственно к реализации латентно-семантического анализа, следует сказать пару слов о предобработке материалов, поступающих на анализ. Для того, чтобы получить не просто набор случайных значений или мешанину из слабо связанных между собой данных, необходимо ввести некоторые определения и ограничения.
Первым делом необходимо определиться с языком, для анализа текстов на котором предназначено наше приложения. Несмотря на то, что латентно-семантический анализ позволяет выявлять скрытые связи между документами независимо от языка, каждый язык требует своего подхода к предварительной обработке документов. В это плане английский язык конечно проще, и из дальнейшего изложения ясно почему, но в силу объективных причин, наше приложение ориентированно на определение темы текстов на русском языке. Ко всему прочему, такой выбор должен облегчить интерпретацию результатов и общее восприятие программы в целом.
На вход нашего приложения подаются тексты или их наборы. И те, и другие обозначим термином «документы». То есть, документами могут быть как фрагменты одного текста, имеющие одну тематику, так и несколько текстов, темы которых пересекаются. При этом в процессе анализа порядок слов в документах или знаки препинания значения не имеют. Во внимание принимаются только отдельные термы и частота их встречаемости в документе. Также важное значение имеет размер документов и их тематика.
Под «термами» здесь понимаются отдельные слова или выражения. Именно термы определяют семантическое значение документов. То есть в текстах одинаковой или смежной тематики определённые термы или группы термов встречаются чаще, чем другие слова и выражения. Ориентируясь по термам в исследуемом материале можно выделить группы документов, имеющих одинаковые или близкие по смыслу темы. При этом считается, что каждое слово имеет единственное значение. Конечно это упрощение, однако без него поставленная задача может оказаться неразрешимой.
Расставив условия можно переходить к описанию самого процесса. Можно подавать на вход как набор документов, и в этом виде он сразу готов к последующей обработке, так и один документ, который буде разбит на части, пригодные для анализа.
Если документ один, то возникает вопрос: каким образом производится разбиение, и вообще, какие документы пригодны для анализа? Здесь многое зависит от размеров исходного документа, от эффективности алгоритмов фильтрации, позволяющих исключить из анализа слова, не несущие смысловой нагрузки, а также от конкретной задачи, которую применение алгоритма призвано решить. Если необходимо определить семантическое значение слова в языке, то объём корпуса текстов должен быть большим – вплоть до нескольких миллионов вхождений терма. Тогда и матрица термы-на-документы будет содержать несколько десятков тысяч термов и документов. Однако в данной работе исследованию подвергаются относительно небольшие тексты в несколько тысяч символов максимум. Соответственно и разбиение производится по абзацам или даже по предложениям, и в частотной матрице будет несколько десятков позиций.
На следующем этапе обработки имеем список, содержащий набор документов. Однако, чтобы применить семантический анализ, требуется произвести ещё несколько операций. Сначала удалим из текстов знаки препинания – для анализа они никакой ценности не представляют, но могут помешать последующим операциям. Удаление осуществляется с помощью регулярных выражений.
После этого из очищенного от точек и запятых текста необходимо удалить так называемые стоп-слова. Это символы, которые не несут никакой смысловой нагрузки – частицы, предлоги, союзы, большинство местоимений и т.д. Оставлять в документах из нельзя, так как это приведёт к сильной зашумлённости итоговых результатов, сделав все последующие манипуляции бесполезными. Так, например, поисковые системы в первую очередь удаляют эти слова из запросов. Полного списка стоп-слов русского языка не существует, так что для решения поставленной задачи использовались несколько списков из различных интернет-источников, и слова, добавленные опытным путём.
Когда в тексте остались только слова, предположительно имеющие смысл, время переходить к следующему этапу обработки – операции стемминга. Стемминг – процесс нахождения основы для заданного слова, найденная основа может не совпадать с корнем слова. Это необходимо, поскольку в русском языке у одного и того же слова может быть много форм в зависимости от склонения, падежа или контекста. Выделение общей основы таких форм позволит значительно сократить количество термов. Важно отметить, что если в английском языке операция стемминга заключается в основном в удалении некоторых окончаний, и корректных результатов можно добиться и вовсе без неё, то в русском языке стемминг является обязательным. И это не удивительно, учитывая количество разнообразных словоформ и принципы словообразования.
В данной работе для стемминга слов применялся стеммер Портера [4]. Этот метод не использует никаких словарей, а выделяет основу слова, следуя определённым правилам отсечения окончаний и суффиксов. Удаление происходит в четыре этапа: на первом отсекаются формообразующие суффиксы частей речи – «ой», «ий», «им» для прилагательных, «а», «и», «я» для существительных и т.д. Затем удаляем букву «и», если она стоит в конце усечённого слова. Далее – словообразующий суффикс и суффиксы превосходных форм.  Между шагами производится проверка полученной основы на соответствие правилам русского языка.
Такой подход был избран прежде всего в силу открытости исходного хода алгоритма стемминга и, следовательно, простоты реализации. Безусловно он не лишён недостатков, главным из которых является чрезмерная «обрезка» некоторых слов, что на результаты, в общем-то, не влияет, но сказывается на восприятии самого слова. Решить проблему можно применив алгоритмы лемматизации или воспользовавшись другими стеммерами. Но следует иметь в виду, что алгоритмы, предназначенные именно для русского языка, сложнее в реализации [5].
Поскольку нашей основной задачей является поиск латентных связей между исследуемыми документами, то из набора документов следует исключить слова, встречающиеся в единственном экземпляре. Такие термы бесполезны для нашего анализа, так как имеют вхождения только в одном из текстов и, следовательно, опираясь на них нельзя установить каких-либо связей между документами. Кроме того, удаление уникальных вхождений позволит существенно сократить ресурсы, необходимые для анализа. С этой же целью при анализе больших объёмов информации удалению подвергаются термы, встречающиеся редко по сравнению со средними значениями. После этого получится список так называемых индексируемых слов, пригодный для дальнейшей работы.
На последнем этапе предобработки требуется построить частотную матрицу индексируемых слов. Это матрица m×n, где m – количество индексированных слов, соответствующих строкам матрицы, а n – количество документов, соответствующих столбцам. Таким образом процесс предобработки текста можно считать завершённым и переходить к следующему этапу – сингулярному разложению.

 Сингулярное разложение

Проделав манипуляции, описанные в предыдущем разделе мы получили матрицу, отражающую частоту встречаемости того или иного терма в каждом из документов. Иными словами, матрица будет содержать весовые коэффициенты для каждого из термов и документов. Теперь к ней следует применить сингулярное разложение [6].
Сингулярное разложение (SVD) позволяет разложить прямоугольную вещественную или комплексную матрицу A(m×n) в произведение трёх матриц (2).

A=U×S×V^T;	(2)

При этом U и V – ортогональные матрицы, а S – диагональная матрица, элементы которых расположены в порядке убывания. Диагональные элементы sk матрицы S называются сингулярными числами. Также, столбцы матрицы U называются левыми сингулярными векторами {uk}, которые формируют ортонормированный базис, а строки матрицы V называются правыми сингулярными векторами {vk} и тоже формируют свой ортонормированный базис (3).

A ˇ=∑_(k=1)^r▒(u_k×s_k×v_k^T ) ;	(3)

Геометрический смысл преобразования состоит в том, что получившийся линейный оператор A ˇ отображает элементы пространства Rm в элементы пространства Rn и представлен в виде последовательно выполняемых линейных операций вращения, растяжения и вращения. То есть, компоненты сингулярного разложения наглядно показывают геометрические изменения при отображении линейным оператором A множества векторов из
одного векторного пространства в другое.
Для того, чтобы понять, как происходит сингулярное разложение, следует обратиться к одному из алгоритмов его реализации. Одним из них является так называемый наивный алгоритм SVD [8]. Для начала матрицуA={a_ij} – она же наша матрица термы-на-документы – нужно представить в виде:

a_ij=∑_(k=1)^r▒〖(u_ik×s_k×v_kj^T )+c_ij 〗;	(4)

Где i = 1,…,m; j = 1,…,n. Значения u_ik×s_k×v_kj^T для данного значения k находятся из минимума выражения (5) при ограничениях нормировки (6).

ε^2=∑_(i=1)^m▒∑_(j=1)^n▒c_ij^2 ;(5)                       ∑_(j=1)^n▒u_ik^2 =∑_(i=1)^m▒v_ik^2 =1;(6)

В матричном виде:
A=U×S×V^T+C;
ε^2=tr(CC^T )=〖∥C∥〗^2;	(7)
U^T U=VV^T=I;

Если значение r достаточно велико, то С=(0). Так будет заведомо при r≥min{m,n}. Минимальное значение r, при
котором выполнимо равенство A = USVT, равно рангу матрицы A.
Далее, для нахождения сингулярного разложения необходимо найти последовательно векторы uk, vk и sk для всех k от 1 до r. В качестве этих векторов берутся нормированные значения векторов ak и bk соответственно:

u_k=a_k/〖∥a∥〗_k ;v_k=b_k/〖∥b∥〗_k ;	(8)

Векторы ak и bk находятся как пределы последовательностей векторов {aki} и {bki}, соответственно a_ki=   lim┬⁡〖〖(a〗_ki 〗) и b_ki=   lim┬⁡〖〖(b〗_ki)〗. Сингулярное число sk находится как произведение норм векторов: s_k=〖∥a〗_k∥×〖∥b〗_k∥ .
Процедура нахождения векторов uk, vk начинается с выбора наибольшей по норме строки b11 матрицы A. Для k = 1 формулы нахождения векторов a1i, b1i имеют вид:
a_1i=(〖Ab〗_1i^T)/(b_1i b_1i^T );b_(1i+1)=(a_1i^T A)/(a_1i^T a_1i );i=1,2…n;	(9)

Для вычисления векторов uk, vk при k = 2,...,r используется вышеприведенная формула (9), c той разницей, что матрица A заменяется на скорректированную на k-м шаге матрицу Ak+1 = Ak − ukskvk.
Таким образом, удалив наименьшие значения сингулярных чисел и проведя обратное преобразование можно убрать шумы из исходных данных. Необходимо оставить в матрицах U, V и S только k наибольших сингулярных значений. При этом матрица A ˇ является наиболее точным приближением матрицы А среди всех матриц с рангом r:

A≈A ˇ= U×S×V^T;	(10)

Можно сказать, что алгоритм снижения шумов при анализе текстов несколько напоминает преобразование Фурье, используемое для снижения шумов, например, при распознавании звуковых сигналов. Однако, если преобразование Фурье имеет заранее заданный набор базисов, относительно которых происходит разложение функции, то здесь базисы выбираются исходя из степени их влияния на итоговую матрицу. Во внимание принимаются те из них, весовые коэффициенты которых оказались наибольшими. Роль сингулярного разложения в латентно-семантическом анализе не ограничивается минимизацией шумов.
Основная идея латентно-семантического анализа состоит в том, что по итогам сингулярного разложения мы получаем результирующую матрицуA ˇ, в которой содержится лишь первые k компонентов исходной матрицы A [7]. Именно эта матрица и будет отражать структуру ассоциативных латентных (скрытых) зависимостей, основой которой служат те самые весовые коэффициенты из частотной матрицы термы-на-документы.
Получившиеся на выходе значения из матриц U, V и S как раз и используются для отображения исходной матрицы А в семантическое пространство. Их можно представить графически в пространстве размерности k. Выбор k зависит от поставленной задачи и подбирается эмпирически. Он зависит от количества исходных документов. Следует иметь в виду, что если k слишком велико, то метод теряет свою эффективность в шумоподавлении, а если слишком мало, то различия между термами и документами перестают улавливаться: остаётся один документ, к которому будут тяготеть все без исключения термы. Поэтому в данной работе значение k выбирается путём анализа главных компонент (PCA) в полученных матрицах.
Можно заметить, что такой подход несколько негибок и точность результатов будет сильно варьироваться в зависимости от изначального объёма текстов и матрицы термы-на-документы. Это так, однако представление в трёхмерном пространстве было выбрано нами потому, что для большинства исследуемых текстов, в силу небольшого объёма в 100-500 символов, такое разложение представляется оптимальным. Кроме того, трёхмерное представление даст нам больше информации для интерпретации результатов и повысит точность результатов. Также следует заметить, что точность не зависит напрямую от количества документов или термов, так что даже при относительно высоком значении k она может неожиданно вырасти до оптимального уровня.
Таким образом, проведя сингулярное разложение частотной матрицы, мы получим в своё распоряжение три матрицы U, V и S, а также матрицуA ˇ. Однако, полученные матрицы по-прежнему будут иметь исходную размерность m×n, что не только повысит сложность и объём дальнейших расчётов, но и сделает невозможным графическое представление результатов. Было бы неплохо отобрать из матриц U, V и S только те признаки, которые наиболее информативны, и в этом нам поможет метод анализа главных компонент (АГК) [12].
В данной работе анализ главных компонент является вспомогательным алгоритмом и подробно описываться не будет. Тем не менее, следует заметить, что суть его состоит в снижении размерности исследуемой выборки. АГК позволит выделить из вектора признаков каждого объекта матриц U, V и S только те признаки, которые влияют на наличие скрытых связей между термами и документами в исследуемой выборке. Таким образом, размерность матриц будет сильно сокращена, что позволит представить выборку графически – на 2-х или 3-х мерном графике. Кроме того, это сильно упростит задачу итоговой классификации.
Итак, описав процесс сингулярного разложения и получив все необходимые данные, можно переходить к завершающим этапам разработки – интерпретации результатов и их визуализации.

 Результаты анализа, их интерпретация и представление

Итак, после проведения сингулярного разложения в нашем распоряжении оказалось четыре матрицы: матрица U с левыми сингулярными векторами, матрица V с правыми сингулярными векторами, матрица S с сингулярными значениями исходной частотной матрицы и матрицаA ˇ, содержащая наиболее точное приближение весовых коэффициентов матрицы термы-на-документы. После проведения анализа главных компонент удалось сократить размерность матриц U и V с n и m компонентов соответственно, до k компонентов. Полученные данные можно подвергнуть заключительному этапу исследования.
Итак, имеем две матрицы размерности k: V – матрицу документов и U – матрицу термов. В результате PCA размеры матриц равны k=2 или k=3, такие данные легко представить в пространстве соответствующей мерности.
Для построения графика следует проделать следующие действия. Во-первых, столбцы матрицы U понадобятся нам для построения точек, отображающих положение термов в семантическом пространстве. Таким же образом построим точки, отражающие положение документов, в качестве координат используя строки из матрицы V. Значения из матриц S иA ˇ будут использованы для дополнительной проверки как корректности анализа, так и интерпретации результатов. К примеру, сингулярные значения матрицы S должны быть упорядочены по убыванию, впрочем, как и значения U и V, если этого не наблюдается, то анализ был проведён неверно, либо исходные данные были некорректными.
Выполнив построение точек в семантическом пространстве получим подобную картину (Рис. 3.1). Здесь синим цветом отмечены документы – пронумерованные фрагменты текста, а красным – термы из него.


Рисунок 3.1 – Семантическое пространство некоторого текста

Построение графика считается промежуточным результатом и служит для дополнительного контроля результатов исследования [9].
После того, как график построен можно переходить непосредственно к задаче классификации. В рамках данной работы классификация состоит в том, чтобы каждому документу была поставлена в соответствие группа термов. Термы выбираются таким образом, чтобы вероятность того, что они правильно отражают содержание данного документа, была наибольшей. Поскольку имеются координаты термов и документов в семантическом пространстве, то для определения таких групп уместно воспользоваться метрическими методами классификации.
Это означает, что определять принадлежность того или иного терма к одному из документов будем, опираясь на расстояние от документа до этих термов. В качестве основного был выбран метод k ближайших соседей. То есть для каждого документа выбирается k ближайших термов. При этом, если исследуемых текстов несколько, то после нахождения ближайших термов задачу можно считать решённой, поскольку все исследуемые документы оказываются классифицированы – каждому присвоен список термов. Если же текст один, а в качестве документов выступают его фрагменты, то после классификации документов должно быть выполнено сведение всех множеств термов в одно, которое и будет считаться тематическим описанием данного текста [11].
Выбранный способ классификации не лишён недостатков, среди которых чувствительность к шуму и возникновение неоднозначности при наличии двух термов с одинаковыми расстояниями до одного документа. Однако, в данном случае на первый недостаток можно не обращать внимания, так как исследуемая выборка практически лишена шумов благодаря предобработке текста и анализу главных компонент. Что касается других недостатков, то от них можно избавиться оптимизировав количества термов – k – с помощью функционала скользящего контроля (11). На текстах небольшого объёма (до 500 слов) k, к примеру, равно 5.

L= ∑_(i=1)^l▒〖[a(x_i 〖; x〗^l\\{x_i },k) ≠y_i]〗;
(11)

Также хорошие результаты могут быть получены с применением метода окна Парзена. То есть в качестве главных будут выбираться k термов, находящихся на расстоянии h от документа. Такой подход годиться для решения задач с неравномерным распределением объектов. Ширина окна может быть фиксированной или зависеть от числа k ближайших соседей, определяемого скользящим контролем (12).  Здесь K – весовая функция, убывающая по мере того, как растёт расстояние до объекта; для k+1-го соседа значение веса равно 0.

a(u,x^l,k,K)=argmax∑_(i=1)^l▒[y_u^((i) )=y]K(p(u,x_u^((i) ) )/p(u,x_u^((k+1) ) ) ) ;	(12)

Таким образом и происходит выбор группы термов, описывающих тематику каждого из исследуемых документов.
Безусловно такой алгоритм выбора тематики не совершенен. Наиболее оптимальным вариантом для реализации латентно-семантического анализа является реализация обучаемости системы. Тогда на основе большого объёма проанализированных тестов можно было бы определять тему исследуемого текста с гораздо большей точность. В этом случаи пользователь, как конечное звено семантического анализа, выполняющее коррекцию результатов, понадобился бы лишь на этапе обучения. Но, как уже было сказано выше, сам по себе алгоритм ЛСА является аналогом простейшей нейронной сети, так что и в таком виде способен давать неплохие результаты. К тому же не исключена реализация упрощённого варианта обучения, основанного только на результатах классификации, но не сингулярного разложения. В дальнейшем также возможно введение полноценной обучаемости данной системы.
На финальном этапе работы программы определяется тематическая принадлежность всех исследуемых документов. Таким образом можно говорит о том, что созданное приложение полностью выполняет поставленную задачу – определяет набор термов, или ключевых слов, которые наилучшим образом отражают тематику текста.
По итогам работы программа предоставляет в распоряжение пользователя все статистические данные, на основании которой была вычислена наиболее вероятная тема исходного документа и записывает их в файл с результатами, а также представляет их графически для облегчения интерпретации [10]. Тем не менее, для осуществления контроля эффективности пользователь должен самостоятельно определить тему текста. В дальнейшем планируется реализация дополнительных методик с целью повышения эффективности работы анализа.
В следующем разделе представлено исследование работы текущей программы и оценка её эффективности.

 Информационная безопасность в приложении

Разумеется, выбор средств разработки накладывает некоторые ограничения на средства, которые будут обеспечивать безопасность в приложении. Однако, поскольку разработка ведётся на языке Python, то это скорее преимущество, чем недостаток. На Python существует множество пакетов, с помощью которых можно внедрить в программу функции для работы с шифрованием. Одним из таких пакетов является модуль PyCrypto, который и будет использован для реализации шифрования. Также используем библиотеку hashlib.
Шифрование документов программа осуществляет с помощью симметричного алгоритма блочного шифрования AES. Этот алгоритм проверен временем и доказал свою эффективность. Его криптостойкость такова, что для взлома требуется либо слишком большое количество зашифрованных документов (порядка 200 миллионов), либо доступа к ОС, на которой выполнялось шифрование. И хотя это не гарантирует стопроцентной защищённости документов, такие гарантии и не входят в поставленную задачу. Для её выполнения выбранного алгоритма более чем достаточно. Кроме того, AES входит в состав большинства специализированных пакетов, в том числе и PyCrypto, что немаловажно.
Поскольку приложение должно соответствовать принципам защищённого документооборота, то в нём должны быть реализованы возможности как считывания зашифрованных файлов, так и создания таковых. С этой целью был написан класс FileCryptor, поддерживающий обе операции.
Для того, чтобы зашифровать текст с помощью алгоритма AES необходимы пароль – ключ и инициализирующий вектор. Размер ключа может составлять 128, 192 или 256 бит. Пароль задаётся пользователем, который, естественно, выберет такой, который он сможет запомнить и который вряд ли будет длинной ровно 128 или 256 бит. Чтобы обойти эту проблему воспользуемся инструментами hashlib. Сгенерируем на основе пароля 256-битный ключ, воспользовавшись для этого хеш-функцией SHA-256. Таким образом пользователь сможет выбрать фактически любой ключ для шифруемого документа, а на качестве шифрования это не отразиться.
Далее необходимо задать инициализирующий вектор - ИВ. ИВ является важной частью алгоритмов блочного шифрования, если использовать один и тот же ИВ для шифрования различных документов, то это сильно облегчит их взлом и расшифровку. Именно поэтому для каждого нового документа генерируется случайный ИВ размера блока AES (128 бит). При этом пользователь никак не взаимодействует с инициализирующим вектором. Даже если ИВ станет известен предполагаемому злоумышленнику, это не повысит его шансы на взлом шифра. Пользователь может даже шифровать разные документы одним ключом, но поскольку ИВ будут различными, то стойкость шифра сохраниться.
Шифрование файла с помощью класса FileCryptor происходит следующим образом:
Пользователь указывает ключ и имя файла, который необходимо зашифровать. Затем программа генерирует ИВ и шифрует передаваемый текст, который при этом разбивается на блоки по 128 бит в каждом. Затем зашифрованный текст вместе с инициализирующим вектором записываются в указанный файл.
Процедура дешифровки происходит аналогично. Указывается ключ и имя файла, далее считывается ИВ, а затем происходит дешифровка содержимого файла.
При работе с приложением класс FileCryptor скрыт от пользователя, к нему нельзя обратиться напрямую. Однако, если на вход подаются зашифрованные файлы или пользователь хочет получить таковые на выходе, то ему достаточно лишь указать свой ключ при направлении команды приложению. Дальнейшие операции производятся автоматически.
