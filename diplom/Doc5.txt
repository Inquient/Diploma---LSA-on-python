4	Описание программного продукта

Хотя данная работа преследует прежде всего исследовательские задачи, одним из результатов её выполнения является создание программного продукта – приложения для латентно-семантического анализа. Требования к программному продукту, а также алгоритмы, лежащие в его основе, были подробно описаны в разделах Технического задания и Обоснования средств разработки. Данный раздел посвящён описанию итоговой программы и принципов её организации.

4.1 Описание структуры программы

Центральным элементом программы является класс LSA. Именно этот скрипт является основным исполняемым файлом приложения и содержит все необходимые операции латентно-семантического анализа, последовательно применяемые к документам. При этом используются методы, определённые в классах TextProcessor, Stemmer и FileCryptor. Скрипт может выполнятся независимо при получении команды из командной строки. В данном случаи процесс его работы выглядит следующим образом.
Первым делом, с помощью библиотеки argparse происходит парсинг передаваемой скрипту команды. Возможные аргументы команды уже были описаны в Техническом задании, однако в ходе разработки возникла необходимость в дополнительных параметрах (1).

LSA.py –s document.txt –d C:\result –e --pca	(1)

В данном примере аргумент «--pca» отвечает за включение анализа главных компонент. Если ключ «--pca» активен, то перед классификацией будет производиться отбор признаков на основе анализа главных компонент. Такой подход полезен не во всех случаях, поэтому его внедрение в программу было выполнено в виде отключаемой опции. Подробнее об эффективности разных подходов к анализу документов рассказано в следующем разделе.
После произведения парсинга и получения необходимых для анализа текстов начинается этап предварительной обработки текста. За это отвечают классы Stemmer и TextProcessor. Их методы позволяют выполнить все необходимые подготовительные операции и построить матрицу термов и документов. Сначала удаляются стоп-символы и стоп-слова по специальным спискам. Затем, с помощью класса Stemmer выполняется выделение основы всех слов. Код этого класса достаточно объёмный, поэтому целиком вынесен в Приложение А.

Таблица 4.1 – Свойства и методы класса Stemmer
_vowel
_non_vowel	Регулярные выражения, содержащие все гласные и все согласные соответственно.
_re_rv	Область исходного слова после первого сочетания "гласная-согласная".
_re_r1	Область _re_rv после первого сочетания "гласная-согласная".
_re_perfective_gerund
_re_adjective
_re_participle
_re_verb и т.д.	Регулярные выражения, содержащие окончания, характерные для различных частей речи русского языка. Например, для существительного: «-ев», «-ов», «-ь», «-ие», «-ии», «-ем» и т.д.
Find_rv	Выполняет поиск в строке _re_rv на предмет специфических окончаний, указанных в переменных класса.
Find_r2	То же в строке _re_r1.
Cut	Обрезает окончание слова, если оно находиться в правильной позиции.
Stem	Последовательно выполняет необходимые операции стемминга.

Далее производится удаление слов, встречающихся в тексте только один раз. За это отвечает метод remove_unique класса TextProcessor. При необходимости число повторов, которое считается минимальным, можно увеличить. Для этого используется параметр скрипта «-f» – минимальная частота встречаемости.
Такая опция введена потому, что если на обработку поступают большие тексты, то даже исключение стоп-слов оставляет много семантического «мусора» в итоговой выборке. Увеличив количественный порог, можно повысить эффективность работы программы.
После этого происходит формирования матрицы термов и документов из полученного массива ключей и частот. Для этого служит метод table_generator класса TextProcessor.

Таблица 4.2 – Методы класса TextProcessor
remove_stopwords	Выполняет удаление всех стоп-слов – предлогов, союзов, слов, не несущих смысловой нагрузки. Полный список стоп-слов содержится в файле stopwords.txt.
remove_symbols	Выполняет удаление всех стоп-символов – знаков препинания – из текста.
remove_unique	Удаление слов, встречающихся в тексте в единственном экземпляре. Позволяет избавиться от слов, слабо влияющих на тему документа.
table_generator	Генерирует матрицу термов и документов по массиву ключевых слов. Также создаёт отображаемую таблицу для отчёта.

Следует заметить, что помимо частотной матрицы, непосредственно используемой программой в дальнейшем, метод генерирует отображаемую таблицу – ту же матрицу, но в пригодном для помещения в отчёт виде. Таблица затем помещается в отчёт об анализе.
Когда в распоряжении приложения оказывается матрица термов и документов, следует следующий этап семантического анализа – сингулярное разложение. Сингулярное разложение в приложении выполнено средствами библиотеки Numpy. На выходе из частотной матрицы получаем три массива с весовыми коэффициентами и координатами термов и документов.

Таблица 4.3 – Сингулярное разложение
1.
2.
3.
4.
5.	LA = numpy.linalg
freqMatrix = numpy.array(table)
print(freqMatrix)
terms, s, docs = LA.svd(freqMatrix, full_matrices=False)
assert numpy.allclose(freqMatrix, numpy.dot(terms, numpy.dot(numpy.diag(s), docs)))

Также производится проверка на предмет соответствия полученных данных основным свойствам сингулярного разложения. После этого идёт построения графика семантического пространства документа на основе данных разложения. Этот этап может предваряться анализом главных компонент для сокращения выборки.

Таблица 4.4 – Формирование графика
1.
2.
3.
4.
5.
6.
7.
8.
9.	if(pca):
    pca = PCA(n_components=3)
    fit_docs = pca.fit_transform(docs)
    print(fit_docs)
    fit_terms = pca.fit_transform(terms)
    print(fit_terms)
    plotAsPCA(fit_docs, fit_terms, keys)
else:
    plotGraphic(freqMatrix, docs, terms, keys)

На данном этапе применяются методы plotAsPCA и plotGraphic, предназначенные для построения выборки, прошедшей анализ главных компонент и обычной выборки соответственно.
Далее результаты расчётов помещаются в словарь statictics. В нём по номеру документа хранятся словари из пар: «терм» – «расстояние от терма до данного документа». Это позволит получить наглядную информацию о расстоянии между термами и документами. Данная информация необходима программе для проведения классификации.
После этого словарь statictics упорядочивается по расстоянию от документа до каждого из термов – вначале идут ближайшие термы с наименьшим расстоянием, а затем более далёкие термы, расстояние до которых велико. Таким образом производится классификация, в результате которой ближайшие к документу термы считаются наиболее подходящими для описания его тематики. Программный код описанных преобразований приведён в Приложении А.
На выходе пользователь получает список ключевых слов для исследуемого документа или списки для набора документов (Рис. 4.1).


Рисунок 4.1 – Результат работы программы

Таким образом, весь процесс работы приложения, а также входные и выходные данные, можно считать описанными. Прежде чем перейти к исследованиям, остаётся лишь сказать несколько слов о взаимодействии пользователя с приложением.

4.2 Использование приложения

Предполагается, что пользователь будет использовать данное приложение по его прямому назначению, то есть для проведения семантического анализа текста с целью выявления ключевых слов, наилучшим образом раскрывающих его тематику. Иными словами, программа будет использоваться как скрипт. В таком случае от пользователя требуется лишь знание основных параметров команды вызова скрипта.

Таблица 4.5 – Параметры команды
-s	Ключ, вслед за которым требуется указать путь к исследуемому документу или папке с документами. Программа способна обрабатывать оба варианта входных данных. Является обязательным параметром.
-d	Параметр –d требует задания папки, в которую будут записаны результаты анализа – график и текстовый документ.
-e	Позволяет включить шифрование для работы с уже зашифрованными файлами, либо для создания таковых. В качестве аргумента принимает пользовательский пароль.
-f	В качестве аргумента передаётся значение минимальной частоты встречаемости слова в документе. Необходим при обработке массивных текстов, где даже не несущие семантической нагрузки слова могут повторяться десятки раз.
--pca	Позволяет включить постобработку результатов сингулярного разложения в виде анализа главных компонент.

Таким образом, в использовании приложения в качестве скрипта нет ничего сложного. Освоить его способен любой пользователь, имеющий навыки работы с командной строкой Windows или терминалом в Linux.
По завершении работы, программа записывает результаты анализа – график и текстовый отчёт – в указанную пользователем папку. При необходимости текстовые документы могут быть зашифрованы.
Кроме того, возможно применение программы в виде библиотеки для внедрения латентно-семантического анализа в другие проекты. С этой целью все файлы приложения были собраны в подключаемый модуль на языке python. Такая организация приложения, ко всему прочему, позволит использовать его основной метод, а также методы других его классов в других проектах в смежной предметной области.
Это расширяет возможности приложения за рамки исследовательских задач, поставленных в данной работе, и является безусловным преимуществом перед аналогичными приложениями.
