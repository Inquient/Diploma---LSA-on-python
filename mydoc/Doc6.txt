В качестве объекта исследования и основного инструмента данной курсовой работы выступает латентно-семантический анализ. И прежде чем переходить к описанию его работы следует обосновать такой выбор.
ЛСА позволяет находить взаимосвязь между документами и терминами, встречающимися в них, ставить некоторую тематику в соответствие документу. Именно эти свойства и послужили ключевыми факторами при выборе. Вообще, работу алгоритма можно сравнить с простейшей нейросетью. В ней три слоя: первый содержит множество термов, второй – набор документов, а третий – скрытый слой с весовыми коэффициентами, связывает первые два. Такая организация работы позволяет говорить не просто о реализации алгоритма, но и о построении простейшего, даже несколько неполноценного ИИ.
Исходной информацией для ЛСА служит матрица термы-на-документы, содержащая частоты встречаемости тех или иных слов в документах. Наиболее распространённый вариант анализа заключается в том, что эта матрица подвергается сингулярному разложению. Полученная на выходе матрица будет отражать основную структуру различных зависимостей, присутствующих в исходной матрице и, следовательно, структуру зависимостей между документами и термами.
Выбранный алгоритм обладает рядом достоинств. Он признан одним из лучших методов для выявления скрытых связей в наборе документов, даже если статистические методы с этим не справляются, что полностью соответствует поставленной задаче. Также ЛСА может применяться как с использованием обучения, так и без него, что как упрощает данную работу, так и позволяет наметить пути совершенствования разрабатываемого приложения. Наконец, метод позволяет игнорировать такие лексические особенности как многозначность слов, что идёт на пользу итоговым результатам.
Конечно ЛСА не лишён и недостатков, самым существенным из которых является небольшая скорость работы алгоритма и значительное её снижение при увеличении объёма входных данных. Скорость вычисления находится по следующей формуле (1), где V – скорость, k – размерность матрицы термы-на-документы, а Ndoc и Nterm – количество документов и термов соответственно.

Кроме того, в процессе вычислений предполагается, что термы и документы имеют нормальное распределение, однако это не соответствует действительности. Ближе всего к реальному распределение Пуассона, в связи с чем для практического применения лучше подходит вероятностный латентно-семантический анализ (ВЛСА). Этот подход считается более обоснованным, но не даёт возможности получить детерминированные результаты – они каждый раз будут отличаться, хотя и незначительно. 
В силу этого в качестве основного алгоритма в данном исследовании был избран чистый ЛСА. Используя его можно добиться наиболее понятных и чётких результатов по сравнению с аналогами. 


	Первичная обработка текста

Прежде чем переходить непосредственно к реализации латентно-семантического анализа, следует сказать пару слов о предобработке материалов, поступающих на анализ. Для того, чтобы получить не просто набор случайных значений или мешанину из слабо связанных между собой данных, необходимо ввести некоторые определения и ограничения.
Первым делом необходимо определиться с языком, для анализа текстов на котором предназначено наше приложения. Несмотря на то, что латентно-семантический анализ позволяет выявлять скрытые связи между документами независимо от языка, каждый язык требует своего подхода к предварительной обработке документов. В это плане английский язык конечно проще, и из дальнейшего изложения ясно почему, но в силу объективных причин, наше приложение ориентированно на определение темы текстов на русском языке. Такой выбор должен облегчить интерпретацию результатов и общее восприятие программы в целом.
На вход нашего приложения подаются тексты или их наборы. И те, и другие обозначим термином «документы». То есть, документами могут быть как отдельные тексты, имеющие одну тематику, так и несколько текстов, темы которых пересекаются. При этом в процессе анализа порядок слов в документах или знаки препинания значения не имеют. Во внимание принимаются только отдельные термы и частота их встречаемости в документе. Также важное значение имеет размер документов и их тематика.
Под термами здесь понимаются отдельные слова или выражения. Именно термы определяют семантическое значение документов. То есть в текстах одинаковой или смежной тематики определённые термы или группы термов встречаются чаще, чем другие слова и выражения. Ориентируясь по термам в исследуемом материале можно выделить группы документов, имеющих одинаковые темы или близкие по смыслу. При этом считается, что каждое слово имеет единственное значение. Конечно это сильное упрощение, однако без него поставленная задача может оказаться неразрешимой.
Расставив условия можно переходить к описанию самого процесса. Итак, на вход программы поступает документ и немедленно возникает вопрос: а что именно туда подаётся, набор текстов или один текст? Работа приложения организованна таким образом, что это не имеет значения. Можно подавать на вход как набор документов, и в этом виде он сразу готов к последующей обработке, так и один документ, который буде разбит на части, пригодные для анализа. 
Если документ один, то возникает вопрос: каким образом производится разбиение, и вообще, какие документы пригодны для анализа? Здесь многое зависит от размеров исходного документа, от эффективности алгоритмов фильтрации, позволяющих исключить из анализа слова, не несущие смысловой нагрузки, а также от конкретной задачи, которую применение алгоритма призвано решить. Если необходимо определить семантическое значение слова в языке, то объём корпуса текстов должен быть большим – вплоть до нескольких миллионов вхождений терма. Тогда и матрица термы-на-документы будет содержать несколько десятков тысяч термов и документов. Однако в данной работе исследованию подвергаются относительно небольшие тексты в несколько тысяч символов. Соответственно и разбиение производится по абзацам или даже по предложениям, и в матрице будет несколько десятков позиций максимум.
На следующем этапе обработки имеем список, содержащий набор документов. Однако, чтобы применить семантический анализ, требуется произвести ещё несколько операций. Сначала удалим из текстов знаки препинания – для анализа они никакой ценности не представляют, но могут помешать последующим операциям. Удаление осуществляется с помощью регулярных выражений.
После этого из очищенного от точек и запятых текста необходимо удалить так называемые стоп-слова. Это символы, которые не несут никакой смысловой нагрузки – частицы, предлоги, союзы, большинство местоимений и т.д. Оставлять в документах из нельзя, так как это приведёт к сильной зашумлённости итоговых результатов, сделав все последующие манипуляции бесполезными. Так, например, поисковые системы в первую очередь удаляют эти слова из запросов. Полного списка стоп-слов русского языка не существует, так что для решения поставленной задачи использовались несколько списков из различных интернет-источников, и слова, добавленные опытным путём.
Когда в тексте остались только слова, предположительно имеющие смысл, время переходить к следующему этапу обработки – операции стемминга. Стемминг – процесс нахождения основы для заданного слова, найденная основа может не совпадать с корнем слова. На сегодняшний день одним из самых распространённых алгоритмов стемминга является алгоритм, написанный Мартином Портером. Существует даже отдельный фреймворк – Snowball, поддерживающий помимо английского стемминг слов и в русском языке. Важно отметить, что если в английском языке операция стемминга заключается в основном в удалении некоторых окончаний, и корректных результатов можно добиться и вовсе без неё, то в русском языке стемминг является обязательным. И это не удивительно, учитывая количество разнообразных словоформ и принципы словообразования.
В данной работе для стемминга слов применялся стеммер Портера. Этот метод не использует никаких словарей, а выделяет основу слова, следуя определённым правилам отсечения окончаний и суффиксов. Удаление происходит в четыре этапа: на первом отсекаются формообразующие суффиксы частей речи – «ой», «ий», «им» для прилагательных, «а», «и», «я» для существительных и т.д. Затем удаляем букву «и», если она стоит в конце усечённого слова. Далее – словообразующий суффикс и суффиксы превосходных форм.  Между шагами производится проверка полученной основы на соответствие правилам русского языка.
Такой подход был избран прежде всего в силу открытости исходного хода алгоритма стемминга и, следовательно, простоты реализации. Безусловно он не лишён недостатков, главным из которых является чрезмерное обрезание некоторых слов, что на результаты, в общем-то, не влияет, но сказывается на восприятии самого слова. Решить проблему можно применив алгоритмы лемматизации или воспользовавшись другими стеммерами. Но следует иметь в виду, что алгоритмы, предназначенные именно для русского языка, сложнее в реализации.
Поскольку нашей основной задачей является поиск латентных связей между исследуемыми документами, то из набора документов следует исключить слова, встречающиеся в единственном экземпляре. Такие термы бесполезны для нашего анализа, так как имеют вхождения только в одном из текстов и, следовательно, опираясь на них нельзя установить каких-либо связей между документами. Кроме того, удаление уникальных вхождений позволит существенно сократить ресурсы, необходимые для анализа. С этой же целью при анализе больших объёмов информации удалению подвергаются термы, встречающиеся редко по сравнению со средними значениями. Но в нашем случае это излишне. После этого получится список так называемых индексируемых слов, пригодный для дальнейшей работы.
На последнем этапе предобработки требуется построить частотную матрицу индексируемых слов. Это матрица m?n, где m – количество индексированных слов, соответствующих строкам матрицы, а n – количество документов, соответствующих столбцам. Таким образом процесс предобработки текста можно считать завершённым и переходить к следующему этапу – сингулярному разложению.

	Сингулярное разложение

Проделав манипуляции, описанные в предыдущем разделе мы получили матрицу, отражающую частоту встречаемости того или иного терма в каждом из документов. Иными словами, матрица будет содержать весовые коэффициенты для каждого из термов и документов. Теперь к ней следует применить сингулярное разложение.
Сингулярное разложение (SVD) позволяет разложить прямоугольную вещественную или комплексную матрицу A(m?n) в произведение трёх матриц (2).
При этом U и V – ортогональные матрицы, а S – диагональная матрица, элементы которых расположены в порядке убывания. Диагональные элементы sk матрицы S называются сингулярными числами. Также, столбцы матрицы U называются левыми сингулярными векторами {uk}, которые формируют ортонормированный базис, а строки матрицы V называются правыми сингулярными векторами {vk} и тоже формируют свой ортонормированный базис (3). 

Геометрический смысл преобразования состоит в том, что получившийся линейный оператор A ?  отображает элементы пространства Rm в элементы пространства Rn и представлен в виде последовательно выполняемых линейных операций вращения, растяжения и вращения. То есть, компоненты сингулярного разложения наглядно показывают геометрические изменения при отображении линейным оператором A множества векторов из
одного векторного пространства в другое.
Для того, чтобы понять, как происходит сингулярное разложение, следует обратиться к одному из алгоритмов его реализации. Одним из них является так называемый наивный алгоритм SVD. Для начала матрицуA={a_ij} – она же наша матрица термы-на-документы – нужно представить в виде:


Где i = 1,…,m; j = 1,…,n. Значения u_ik  ?s_k  ?v_kj^T для данного значения k находятся из минимума выражения (5) при ограничениях нормировки (6).


?
В матричном виде:


Если значение r достаточно велико, то С=(0). Так будет заведомо при r?min?{m,n}. Минимальное значение r, при
котором выполнимо равенство A = USVT, равно рангу матрицы A. 
Далее, для нахождения сингулярного разложения необходимо найти последовательно векторы uk, vk и sk для всех k от 1 до r. В качестве этих векторов берутся нормированные значения векторов ak и bk соответственно:



Векторы ak и bk находятся как пределы последовательностей векторов {aki} и {bki}, соответственно a_ki=   limT???(a?_ki ?) и b_ki=   limT???(b?_ki)?. Сингулярное число sk находится как произведение норм векторов: s_k= ??a?_k????b?_k? .
Процедура нахождения векторов uk, vk начинается с выбора наибольшей по норме строки b11 матрицы A. Для k = 1 формулы нахождения векторов a1i, b1i имеют вид:

Для вычисления векторов uk, vk при k = 2,...,r используется вышеприведенная формула (9), c той разницей, что матрица A заменяется на скорректированную на k-м шаге матрицу Ak+1 = Ak ? ukskvk.
Таким образом, удалив наименьшие значения сингулярных чисел и проведя обратное преобразование можно убрать шумы из исходных данных. Необходимо оставить в матрицах U, V и S только k наибольших сингулярных значений. При этом матрица A ?  является наиболее точным приближением матрицы А среди всех матриц с рангом r:


Можно сказать, что алгоритм снижения шумов при анализе текстов несколько напоминает преобразование Фурье, используемое для снижения шумов, например, при распознавании звуковых сигналов. Однако, если преобразование Фурье имеет заранее заданный набор базисов, относительно которых происходит разложение функции, то здесь базисы выбираются исходя из степени их влияния на итоговую матрицу. Во внимание принимаются те из них, весовые коэффициенты которых оказались наибольшими. Роль сингулярного разложения в латентно-семантическом анализе не ограничивается минимизацией шумов.
Основная идея латентно-семантического анализа состоит в том, что по итогам сингулярного разложения мы получаем результирующую матрицу A ?, в которой содержится лишь первые k компонентов исходной матрицы A. Именно эта матрица и будет отражать структуру ассоциативных латентных (скрытых) зависимостей, основой которой служат те самые весовые коэффициенты из частотной матрицы термы-на-документы.
Получившиеся на выходе значения из матриц U, V и S как раз и используются для отображения исходной матрицы А в семантическое пространство. Их можно представить графически в пространстве размерности k. Выбор k зависит от поставленной задачи и подбирается эмпирически. Он зависит от количества исходных документов. Следует иметь в виду, что если k слишком велико, то метод теряет свою эффективность в шумоподавлении, а если слишком мало, то различия между термами и документами перестают улавливаться: остаётся один документ, к которому будут тяготеть все без исключения термы. Поэтому в данной работе значение k выбрано равным трём, что соответствует примерно 20-30% от общего числа диагональных значений.
Можно заметить, что такой подход крайне негибок и точность результатов будет сильно варьироваться в зависимости от изначального объёма текстов и матрицы термы-на-документы. Это так, однако представление в трёхмерном пространстве было выбрано нами потому, что для большинства исследуемых текстов, в силу небольшого объёма в 100-500 символов, такое разложение представляется оптимальным. Кроме того, трёхмерное представление даст нам больше информации для интерпретации результатов и повысит точность результатов. Также следует заметить, что точность не зависит напрямую от количества документов или термов, так что даже при относительно высоком значении k она может неожиданно вырасти до оптимального уровня.
Таким образом, проведя сингулярное разложение частотной матрицы, мы получим в своё распоряжение три матрицы U, V и S, а также матрицу A ?. Компоненты матриц U и V, сокращённые до трёх столбцов и строк соответственно понадобятся нам для построения координат точек, отображающих термы и документы в трёхмерное пространство. Посчитав расстояние между ними, можно получит весовые коэффициенты, отражающие зависимости термов и документов между собой. Также информацию об этом можно получить из матрицы A ?. 
Итак, описав процесс сингулярного разложения и получив все необходимые данные, можно переходить к завершающим этапам разработки – визуализации результатов и их интерпретации.


	Результаты анализа, их представление и интерпретация

Итак, после проведения сингулярного разложения в нашем распоряжении оказалось четыре матрицы: матрица U с левыми сингулярными векторами, матрица V с правыми сингулярными векторами, матрица S с сингулярными значениями исходной частотной матрицы и матрица A ?, содержащая наиболее точное приближение весовых коэффициентов матрицы термы-на-документы.
Для того, чтобы полученные результаты были наиболее корректно и правильно их визуализировать, тем более, что такая возможность существует. Под визуализацией мы будем понимать графическое отображение результатов анализа. Построение графика необходимо ещё и потому, что на данном этапе в программе отсутствует какая-либо система для обучения или хотя-бы систематизации полученной информации. Это значить, что единственным критерием эффективности работы алгоритма выступает пользовательская оценка, что в свою очередь требует от разработчиков представления результатов в максимально точной и удобной для пользователя форме. Например, графически.
Для построения графика следует проделать следующие действия. Во-первых, столбцы матрицы U числом k понадобятся нам для построения точек, отображающих положение термов в семантическом пространстве. Таким же образом построим точки, отражающие положение документов, в качестве координат используя первые k строки из матрицы V. Значения из матриц S и A ? будут использованы для дополнительной проверки как корректности анализа, так и интерпретации результатов. К примеру, сингулярные значения матрицы S должны быть упорядочены по убыванию, впрочем, как и значения U и V, если этого не наблюдается, то анализ был проведён неверно, либо исходные данные были некорректными.
Выполнив построение точек в семантическом пространстве получим подобную картину:

 

Построение можно осуществлять как в трёхмерном, так и в двухмерном пространстве.
Помимо графиков в распоряжение пользователя поступает листинг с таблицами, к который входят все результирующие матрицы сингулярного разложения, матрица термы-на-документы, список термов и таблицу с расстояниями до всех термов от каждого документа. Также в качестве темы исследуемого текста выбирается термы с наибольшими значениями своих весовых коэффициентов. 
Однако такой алгоритм выбора тематики не совершенен. Наиболее оптимальным вариантом для латентно-семантического анализа является реализация обучаемости системы. Тогда на основе большого объёма проанализированных тестов программа смогла бы определять тему исследуемого текста с гораздо большей точность. В этом случаи пользователь, как конечное звено семантического анализа, выполняющее коррекцию результатов, понадобился бы лишь на этапе обучения. Но реализация машинного обучения представляет собой достаточно трудоёмкую задачу, поэтому в ходе данной работы от этого решено было отказаться в пользу более детального изучения принципов работы самого алгоритма ЛСА.
Итак, по итогам работы программа предоставляет в распоряжение пользователя всю информацию, на основании которой была вычислена наиболее вероятная тема исходного документа, а также представляет его графически для облегчения интерпретации. Тем не менее, для осуществления контроля эффективности пользователь должен самостоятельно определить тему текста. В дальнейшем планируется добавит возможность обучения системы с целью повышения эффективности её работы.
Исследование работы текущей программы и оценка её эффективности представлена в следующем разделе.
